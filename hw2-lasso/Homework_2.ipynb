{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Lasso Regression\n",
    "\n",
    "## 1  Data Set and Programming Problem Overview\n",
    "\n",
    "Nothing to be done here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Ridge Regression\n",
    "\n",
    "Unless if I misinterpreted the task, I had the impression the given code already solved the homework (apart from the interpretation of the results.)\n",
    "\n",
    "### 2.1 $\\lambda$ minimising empirical risk\n",
    "\n",
    "$\\lambda$ | test score | training score\n",
    "---|---|---\n",
    "0.000100   | 0.171345     | 0.006774\n",
    "0.000316   | 0.168880     | 0.006951\n",
    "0.001000   | 0.162705     | 0.008285\n",
    "0.003162   | 0.151900     | 0.014953\n",
    "0.010000   | 0.141887     | 0.032767\n",
    "0.031623   | 0.139648     | 0.060554\n",
    "0.100000   | 0.144566     | 0.094953\n",
    "\n",
    "![Validation performance in function of lambda](lambda_scores.png \"Validation performance in function of lambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prediction Functions\n",
    "\n",
    "![Prediction functions for different values of lambda](prediction_functions.png \"Prediction functions for different values of lambda\")\n",
    "\n",
    "![Coeffictions for different values of lambda](compare_coefs.png \"Coefficients for different values of lambda\")\n",
    "\n",
    "In the ground truth function, most of the coefficients are zero, while a small number of them are non-zero. When there is no regularisation going on the coefficients somewhat mimick this behaviour, i.e. most coefficients are close to zero, while only a small number of them are quite big.\n",
    "\n",
    "Once the regularisation parameter $\\lambda$ is non-zero, the pattern of the coefficients changes. As big coefficients are penalised, the result tends to consist of multiple smaller coefficients yielding the same result. E.g. to get the prediction function to jump up by 1, we could have coefficients $[1, 0]$ with an attached cost of $lambda$, or we could have equivalent coefficients (given there are no data points in between) $[0.5, 0.5]$, with an attached cost of $0.5 \\lambda$. This explains why a higher regularisation parameter causes the parameter values to be smeared out.\n",
    "\n",
    "I would suggest that in order to get a prediction function that looks more similar to the ground truth function, the regularisation term should be of the form $\\lambda 1(w_i \\ne 0)$. This would try to minimise the number of non-zero parameters. However it might be harder to optimise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Coefficient Sparsity Prediction\n",
    "\n",
    "Let's have a look at the confusion matrices of the coefficients to check whether we can predict non-zero ground truth coefficients.\n",
    "\n",
    "zero threshold: 1e-06\n",
    "\n",
    "_   | 1 | 0   \n",
    "---|---|---  \n",
    "0  | 201 |\t 189\n",
    "1  |  7  | 3\n",
    "\n",
    "zero threshold: 0.001\n",
    "\n",
    "_   | 1 | 0   \n",
    "---|---|---  \n",
    "0 | 205 | \t 185\n",
    "1 | 7 \t| 3\n",
    "\n",
    "zero threshold: 0.1\n",
    "\n",
    "_   | 1 | 0   \n",
    "---|---|---  \n",
    "0 | 346 |\t 44\n",
    "1 |  8 \t| 2\n",
    "\n",
    "What can be seen from these confusion matrixes is that the non-zeroness of the ground truth coefficients and the predicted coefficients don't seem to correlate. This matches our observations in 2.2.3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
