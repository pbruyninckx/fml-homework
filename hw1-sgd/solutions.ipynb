{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Ridge Regression, Gradient Descent, and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Linear Regression\n",
    "### 2.2 Gradient Descent Setup\n",
    "\n",
    "#### 1. Objective Function\n",
    "$$ J(\\theta) = \\frac{1}{m} \\left( X \\theta - y \\right)^T \\left( X \\theta - y \\right) $$\n",
    "#### 2. Gradient of $J$\n",
    "$$ \\nabla J(\\theta) = \\frac{2}{m} X^T(X\\theta-y) $$\n",
    "#### 3. Approximate change in objective function\n",
    "$$ J(\\theta + \\eta h) - J(\\theta) \\approx - \\eta \\nabla J(\\theta)^T \\nabla J(\\theta)$$\n",
    "#### 4. Update $\\theta$\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\theta & \\leftarrow \\theta + \\eta h \\\\\n",
    "& = \\theta - \\eta \\nabla J(\\theta)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Batch Gradient Descent\n",
    "#### 2 Experiment with step size\n",
    "![Loss function evolution with different step sizes](loss_different_step_sizes.svg)\n",
    "\n",
    "As the plot shows, the loss function diverges for the selected values where $\\eta > 0.05$. It converges for smaller values of $\\eta$, although when $\\eta$ gets smaller, the convergion happens slower.\n",
    "\n",
    "#### 3 Implement backtracking line search\n",
    "![Loss function with good step size and BTL](loss_backtracking_line_search.svg)\n",
    "\n",
    "When using backtracking line search, the loss function  decreases much more per iteration compared to using a fixed step size.\n",
    "However, the loss function is computed a lot more often per step size. To get the same value for the loss function with a fixed step size $\\eta=0.05$ after 1000 iterations, backtracking line search needed less than 500 iterations, but it needed almost 1800 computations of the loss function. While additional rules (and parameters) to update $\\alpha$ less often might help to improve things, it doesn't seem to be worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Ridge Regression (i.e. Linear Regression with $l_2$ regularisation)\n",
    "#### 1. Objective, gradient, and updating\n",
    "Objective:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\left( X \\theta - y \\right)^T \\left( X \\theta - y \\right) + \\lambda\\theta^T\\theta $$\n",
    "Gradient:\n",
    "$$ \\nabla J(\\theta) = \\frac{2}{m} X^T(X\\theta-y) + 2\\lambda\\theta $$\n",
    "Updating:\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta) $$\n",
    "\n",
    "#### 4. Bias Term Regularisation\n",
    "If $B$ is large, the matching coefficient $\\theta_0$ only needs to be small to generate a fairly large bias term $b = B \\theta_0$. On the other hand, if $B$ is small, the matching coefficient $\\theta_0$ needs to be much bigger to generate the same bias term. As the loss term contains $\\theta_0^2$, the smaller that $\\theta_0$ needs to be (especially compared to the other values in the vector $\\theta$, the less impact the regularisation will have. However, to generate any kind of bias, $\\theta_0$ will need to be larger than 0, so there will always be some kind of regularisation on the bias term when using the $B$ approach (unlike excluding the bias term from the loss function).\n",
    "\n",
    "#### 7. Finding the best regularisation value\n",
    "The plot below show the loss on the training data and on the test data, for a range of values of the regularisation parameter $\\lambda$. When the regularisation parameter is small, the model tends to overfit, resulting in low values for loss on the traning data, while the loss values for the test data are quite high. When the regularisation value is too big, the model is underfitting, and the loss is high both on the test set and the training set.\n",
    "![Loss function on train set and test set in function of lambda](regularization.svg)\n",
    "\n",
    "#### 8. Optimal $\\theta$ for deployment\n",
    "For deployment I'd select the vector $\\theta$ associated with the lowest loss on the test set.\n",
    "Hence for this particular problem I'd set $\\lambda = 0.025$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
