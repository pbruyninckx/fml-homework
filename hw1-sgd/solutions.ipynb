{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Ridge Regression, Gradient Descent, and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Linear Regression\n",
    "### 2.2 Gradient Descent Setup\n",
    "\n",
    "#### 1. Objective Function\n",
    "$$ J(\\theta) = \\frac{1}{m} \\left( X \\theta - y \\right)^T \\left( X \\theta - y \\right) $$\n",
    "#### 2. Gradient of $J$\n",
    "$$ \\nabla J(\\theta) = \\frac{2}{m} X^T(X\\theta-y) $$\n",
    "#### 3. Approximate change in objective function\n",
    "$$ J(\\theta + \\eta h) - J(\\theta) \\approx - \\eta \\nabla J(\\theta)^T \\nabla J(\\theta)$$\n",
    "#### 4. Update $\\theta$\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\theta & \\leftarrow \\theta + \\eta h \\\\\n",
    "& = \\theta - \\eta \\nabla J(\\theta)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Batch Gradient Descent\n",
    "#### 2 Experiment with step size\n",
    "![Loss function evolution with different step sizes](loss_different_step_sizes.svg)\n",
    "\n",
    "As the plot shows, the loss function diverges for the selected values where $\\eta > 0.05$. It converges for smaller values of $\\eta$, although when $\\eta$ gets smaller, the convergion happens slower.\n",
    "\n",
    "#### 3 Implement backtracking line search\n",
    "![Loss function with good step size and BTL](loss_backtracking_line_search.svg)\n",
    "\n",
    "When using backtracking line search, the loss function  decreases much more per iteration compared to using a fixed step size.\n",
    "However, the loss function is computed a lot more often per step size. To get the same value for the loss function with a fixed step size $\\eta=0.05$ after 1000 iterations, backtracking line search needed less than 500 iterations, but it needed almost 1800 computations of the loss function. While additional rules (and parameters) to update $\\alpha$ less often might help to improve things, it doesn't seem to be worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Ridge Regression (i.e. Linear Regression with $l_2$ regularisation)\n",
    "#### 1. Objective, gradient, and updating\n",
    "Objective:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\left( X \\theta - y \\right)^T \\left( X \\theta - y \\right) + \\lambda\\theta^T\\theta $$\n",
    "Gradient:\n",
    "$$ \\nabla J(\\theta) = \\frac{2}{m} X^T(X\\theta-y) + 2\\lambda\\theta $$\n",
    "Updating:\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta) $$\n",
    "\n",
    "#### 4. Bias Term Regularisation\n",
    "If $B$ is large, the matching coefficient $\\theta_0$ only needs to be small to generate a fairly large bias term $b = B \\theta_0$. On the other hand, if $B$ is small, the matching coefficient $\\theta_0$ needs to be much bigger to generate the same bias term. As the loss term contains $\\theta_0^2$, the smaller that $\\theta_0$ needs to be (especially compared to the other values in the vector $\\theta$, the less impact the regularisation will have. However, to generate any kind of bias, $\\theta_0$ will need to be larger than 0, so there will always be some kind of regularisation on the bias term when using the $B$ approach (unlike excluding the bias term from the loss function).\n",
    "\n",
    "#### 7. Finding the best regularisation value\n",
    "The plot below show the loss on the training data and on the test data, for a range of values of the regularisation parameter $\\lambda$. When the regularisation parameter is small, the model tends to overfit, resulting in low values for loss on the traning data, while the loss values for the test data are quite high. When the regularisation value is too big, the model is underfitting, and the loss is high both on the test set and the training set.\n",
    "![Loss function on train set and test set in function of lambda](regularization.svg)\n",
    "\n",
    "#### 8. Optimal $\\theta$ for deployment\n",
    "For deployment I'd select the vector $\\theta$ associated with the lowest loss on the test set.\n",
    "Hence for this particular problem I'd set $\\lambda = 0.025$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Stochastic Gradient Descent\n",
    "#### 1. Objective function equivalence\n",
    "Given that\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x_i) - y_i \\right)^2 + \\lambda\\theta^T\\theta \\qquad.\n",
    "$$\n",
    "When we take\n",
    "$$\n",
    "f_i(\\theta) =  \\left(h_\\theta(x_i) - y_i \\right)^2 + \\lambda\\theta^T\\theta \\qquad,\n",
    "$$\n",
    "then\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m f_i(\\theta) \\qquad.\n",
    "$$\n",
    "\n",
    "#### 2. Stochastic gradient is an unbiased estimator\n",
    "\n",
    "When writing out the estimated value, we can proof we have an unbiased estimator\n",
    "using the law of big numbers, and the equivalence from the previous paragraph.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\nabla f_i(\\theta)] & = \\frac{1}{m}\\sum_{i=1}^m \\nabla f_i(\\theta) \\\\\n",
    "& = \\nabla \\left( \\frac{1}{m}\\sum_{i=1}^m f_i(\\theta) \\right) \\\\\n",
    "& = \\nabla J(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 3. SGD Update Rule\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta & \\leftarrow \\theta - \\eta \\nabla f_i(\\theta) \\\\\n",
    "& = \\theta - \\eta \\nabla \\left( \\left(h_\\theta(x_i) - y_i \\right)^2 + \\lambda\\theta^T\\theta \\right) \\\\\n",
    "& = \\theta - \\eta \\nabla \\left( \\left(\\theta^T x_i - y_i \\right)^2 + \\lambda\\theta^T\\theta \\right) \\\\\n",
    "& = \\theta - \\eta \\left( 2 x_i \\left(\\theta^T x_i - y_i \\right) + 2\\lambda\\theta \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 5. Experimentation with stochastic gradient descent.\n",
    "\n",
    "I did a couple of experiments with stochastic gradient descent (SGD). The first of them\n",
    "were with a fixed $\\eta_t$. In these cases the value of $J(\\theta)$ converges, but it converges\n",
    "to a (much) higher value than when using batch gradient descent. The chosen value of $\\eta_t$\n",
    "also determines how good the convergion is.\n",
    "\n",
    "When taking $\\eta_t=\\frac{1}{t}$ the result seems to be much better, in this case the algorithm\n",
    "converges slower, and to a value that is closer to the optimal reached by batch gradient descent.\n",
    "\n",
    "Finally when using $\\eta_t=\\frac{1}{\\sqrt{t}}$ the algorithm converges slower, but it keeps converging\n",
    "for a longer time. For my chosen number of iterations the result was not as good as when using\n",
    "$\\eta_t=\\frac{1}{t}$, but I could imagine that with more time the final result might actually be better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
